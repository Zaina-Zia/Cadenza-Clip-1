"""Evaluate the predictions against the ground truth correctness values."""

import json
import logging
from pathlib import Path

import hydra
import numpy as np
import pandas as pd
from omegaconf import DictConfig
from scipy.stats import kendalltau, pearsonr

logger = logging.getLogger(__name__)


def rmse_score(x: np.ndarray, y: np.ndarray) -> float:
    """Compute the root mean squared error between two arrays."""
    return np.sqrt(np.mean((x - y) ** 2))


def ncc_score(x: np.ndarray, y: np.ndarray) -> float:
    """Compute the normalized cross correlation between two arrays."""
    return pearsonr(x, y)[0]


def kt_score(x: np.ndarray, y: np.ndarray) -> float:
    """Compute the Kendall's tau correlation between two arrays."""
    return kendalltau(x, y)[0]


def std_err(x: np.ndarray, y: np.ndarray) -> float:
    """Compute the standard error between two arrays."""
    return np.std(x - y) / np.sqrt(len(x))


def compute_scores(predictions, labels) -> dict:
    """Compute the scores for the predictions."""
    return {
        "RMSE": rmse_score(predictions, labels),
        "Std": std_err(predictions, labels),
        "NCC": ncc_score(predictions, labels),
        "KT": kt_score(predictions, labels),
    }


# pylint: disable = no-value-for-parameter
@hydra.main(config_path="configs", config_name="config", version_base=None)
def evaluate(cfg: DictConfig) -> None:
    """Evaluate the predictions against the ground truth correctness values."""

    logger.info("Loading ground-truth metadata (train/val split)...")

    # Use the train/val metadata file instead of evalai
    metadata_file = (
        Path(cfg.data.cadenza_data_root)
        / cfg.data.dataset
        / "metadata"
        / f"{cfg.data.dataset}.trainval_val_metadata.json"
    )

    with open(metadata_file, encoding="utf-8") as fp:
        records = json.load(fp)

    record_index = {record["signal"]: record for record in records}

    # Load the predictions file (generated by predict_train_val.py)
    predictions_path = (
        Path(cfg.data.cadenza_data_root)
        / cfg.data.dataset
        / cfg.baseline.predictions_file
    )

    df = pd.read_csv(predictions_path)

    # Normalize column names
    df.columns = [c.strip().lower() for c in df.columns]
    if "signal_id" in df.columns:
        df.rename(columns={"signal_id": "signal"}, inplace=True)
    if "intelligibility_score" in df.columns:
        df.rename(columns={"intelligibility_score": "predicted"}, inplace=True)

    if "signal" not in df.columns or "predicted" not in df.columns:
        raise ValueError(
            f"Predictions file must contain 'signal' and 'predicted' columns. Found: {df.columns.tolist()}"
        )

    # Add correctness from metadata
    df["correctness"] = [record_index[s]["correctness"] for s in df.signal]

    # Compute evaluation metrics
    scores = compute_scores(df["predicted"], df["correctness"])

    # Save results to JSONL file (per baseline)
    output_file = (
        Path(cfg.data.cadenza_data_root)
        / cfg.data.dataset
        / f"{cfg.data.dataset}.{cfg.baseline.name}.trainval.val.evaluate2.jsonl"
    )

    with open(output_file, "a", encoding="utf-8") as fp:
        fp.write(json.dumps(scores) + "\n")

    # Print results clearly
    logger.info(f"Evaluation completed for baseline: {cfg.baseline.name}")
    print(json.dumps(scores, indent=2))
    logger.info(f"Results saved to: {output_file}")


if __name__ == "__main__":
    evaluate()
